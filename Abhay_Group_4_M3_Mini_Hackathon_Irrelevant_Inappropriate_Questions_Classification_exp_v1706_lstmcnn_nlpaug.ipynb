{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhialag/iiscdlfa_kaggle_grp4/blob/main/Abhay_Group_4_M3_Mini_Hackathon_Irrelevant_Inappropriate_Questions_Classification_exp_v1706_lstmcnn_nlpaug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "\n",
        "### Mini Project Notebook: Irrelevant/inappropriate Questions Classification using Deep Neural Networks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maritime-miami"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nljJR6CwfZN_"
      },
      "source": [
        "At the end of the mini-hackathon, you will be able to :\n",
        "\n",
        "* perform data preprocessing/preprocess the text\n",
        "* represent the text/words using the pretrained word embeddings - Word2Vec/Glove\n",
        "* build the deep neural networks to classify the questions as Irrelevant/inappropriate or not\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL0Ve1abn6YJ"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The challenge in this competition is to predict whether a question asked on a well known public forum/platform is irrelevant/inappropriate or not.\n",
        "\n",
        "A irrelevant/inappropriate question is defined as a question intended to make a statement and not with a purpose of looking for helpful/meaningful answers. The following are some of the characteristics that can signify that a question is irrelevant/inappropriate:\n",
        "\n",
        "* Based on false information, or contains absurd assumptions\n",
        "* Does not have a non-neutral tone\n",
        "* Has an exaggerated tone to underscore a point about a group of people\n",
        "* Is rhetorical and meant to imply a statement about a group of people\n",
        "* Is disparaging or inflammatory against an individual or a group of people\n",
        "* Uses sexual content (such as incest, pedophilia), and not to seek genuine answers\n",
        "* Suggests a discriminatory idea against a protected class of people, or seeks confirmation of a stereotype\n",
        "* Based on an unrealistic premise about a group of people\n",
        "* Is not grounded in reality\n",
        "\n",
        "The training dataset includes the questions 1044897 that was asked, and whether it was identified as irrelevant/inappropriate (target = 1) or as relevant/appropriate (target = 0). The test dataset consists of approximately 261000 questions.\n",
        "\n",
        "The training data might be imbalanced or noisy. They are not guaranteed to be perfect. Please take the necessary actions/steps while building the model.\n",
        "\n",
        "\n",
        "## Description\n",
        "\n",
        "This dataset has the following information:\n",
        "\n",
        "1. **qid** - unique question identifier\n",
        "2. **question_text** - the text of the question asked in the well known public forum/platform\n",
        "3. **target** - a question labeled \"irrelevant/inappropriate\" has a value of 1, otherwise 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih-oasWmdZul"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfWGmjNHdZul"
      },
      "source": [
        "To perform classification of approximately 261000 questions asked on a well known public form using Deep Neural Networks such as RNN/CNN/BERT/LSTM as 'irrelevant/inappropriate' questions or 'relevant/appropriate' questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BQEA97zTlTa"
      },
      "source": [
        "## Grading = 10 Marks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdYmy-tJgURN"
      },
      "source": [
        "Here is a handy link to Kaggle's competition documentation (https://www.kaggle.com/docs/competitions), which includes, among other things, instructions on submitting predictions (https://www.kaggle.com/docs/competitions#making-a-submission)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8OapRtHgLnU"
      },
      "source": [
        "## Instructions for downloading train and test dataset from Kaggle API are as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO2jS73oLnCR"
      },
      "source": [
        "### 1. Create an API key in Kaggle.\n",
        "\n",
        "To do this, go to the competition site on Kaggle at (https://www.kaggle.com/t/bde6f23028154933a99e4b4ca8a3dff2) and click on user then click on your profile as shown below. Click Account.\n",
        "\n",
        "![alt text](https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Capture-NLP.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkzGffHdbwX2"
      },
      "source": [
        "### 2. Next, scroll down to the API access section and click on **Create New Token** to download an API key (kaggle.json).\n",
        "\n",
        "![alt text](https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Capture-NLP_1.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtETuXx8b-OC"
      },
      "source": [
        "### 3. Upload your kaggle.json file using the following snippet in a code cell:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "-1pfXBDxWl0Y",
        "outputId": "54330503-f701-474e-9a85-2d006c2f72c0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-867f384a-54c8-4b0d-a2ca-2d4bcbc39c6d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-867f384a-54c8-4b0d-a2ca-2d4bcbc39c6d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"abhaykumardnnai\",\"key\":\"1716a26a8649843ef484f1b554327b9f\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCV_T6MMW4eX",
        "outputId": "9d7a9529-7c59-4354-b5bb-263f614c64ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "#If successfully uploaded in the above step, the 'ls' command here should display the kaggle.json file.\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVZuqBHEghsE",
        "outputId": "be91b481-6b4c-44fe-9f3b-225016b13116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting urllib3==1.25\n",
            "  Downloading urllib3-1.25-py2.py3-none-any.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.9/149.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'urllib3' candidate (version 1.25 at https://files.pythonhosted.org/packages/81/9b/715e96377cc1f87e71d9d4259c6f88bf561a539622ba3042e73188e0bc2d/urllib3-1.25-py2.py3-none-any.whl (from https://pypi.org/simple/urllib3/) (requires-python:>=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, <4))\n",
            "Reason for being yanked: Broken release\u001b[0m\u001b[33m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3173, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4959, in parseImpl\n",
            "    loc, tokens = self_expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 5226, in parseImpl\n",
            "    return super().parseImpl(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4375, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4375, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 821, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 2336, in parseImpl\n",
            "    def parseImpl(self, instring, loc, doActions=True):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1732, in isEnabledFor\n",
            "    return self._cache[level]\n",
            "KeyError: 50\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1523, in critical\n",
            "    if self.isEnabledFor(CRITICAL):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1734, in isEnabledFor\n",
            "    _acquireLock()\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 226, in _acquireLock\n",
            "    _lock.acquire()\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install urllib3==1.25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbukdzJ6cE32"
      },
      "source": [
        "### 4. Install the Kaggle API using the following command\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMj1n1MJcqzN",
        "outputId": "03423101-8239-4d49-d674-de3b6fc6ef35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/59.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.8/118.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for slugify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchdata 0.6.1 requires urllib3>=1.25, but you have urllib3 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U -q kaggle==1.5.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vpy9P1nchhd"
      },
      "source": [
        "### 5. Move the kaggle.json file into ~/.kaggle, which is where the API client expects your token to be located:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yQbPsDOLZ0b4"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BenAWlpI73sm",
        "outputId": "de1cd970-9ad9-4a6b-ae38-cb5df0e8547c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json\n"
          ]
        }
      ],
      "source": [
        "# Execute the following command to verify whether the kaggle.json is stored in the appropriate location: ~/.kaggle/kaggle.json\n",
        "!ls ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vm2jGsCradOS"
      },
      "outputs": [],
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json # run this command to ensure your Kaggle API token is secure on colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32unPZKzdI72"
      },
      "source": [
        "### 6. Now download the Test Data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppuy5gRKHFwv"
      },
      "source": [
        "**NOTE: If you get a '404 - Not Found' error after running the cell below, it is most likely that the user (whose kaggle.json is uploaded above) has not 'accepted' the rules of the competition and therefore has 'not joined' the competition.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41-ETZCE_A1j"
      },
      "source": [
        "If you encounter **401-unauthorised** download latest **kaggle.json** by repeating steps 1 & 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY40TmgfHq0s",
        "outputId": "eeafd7f2-86e2-4856-ae3f-2a50ec2432b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading toxic-questions-classification.zip to /content\n",
            " 81% 49.0M/60.6M [00:00<00:00, 86.6MB/s]\n",
            "100% 60.6M/60.6M [00:00<00:00, 95.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "#If you get a forbidden link, you have most likely not joined the competition.\n",
        "!kaggle competitions download -c toxic-questions-classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvKRiFNglvpC",
        "outputId": "14ccc585-ef80-4652-de05-9c5b6d111272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/toxic-questions-classification.zip\n",
            "  inflating: sample_submission.csv   \n",
            "  inflating: test_dataset.csv        \n",
            "  inflating: train_dataset.csv       \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/toxic-questions-classification.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeKon2vruI_c"
      },
      "source": [
        "## YOUR CODING STARTS FROM HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytHuyujqh5Wg",
        "outputId": "ee60e448-c897-4109-f5b9-b8afe14bdbc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abstract-stocks"
      },
      "source": [
        "## Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O5RcxwQUku6x"
      },
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, concatenate, Dense, Bidirectional, Dropout, Flatten, Conv1D, MaxPooling1D\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pickle\n",
        "from sklearn.metrics import f1_score\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge-4atfoiWxP",
        "outputId": "9f9265ef-fcdd-4c71-9cce-0a9ab8e78ff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-17 11:12:02--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-06-17 11:12:02--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-06-17 11:12:02--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2023-06-17 11:14:41 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ],
      "source": [
        "# @title Download the glove embedding Dataset\n",
        "# !wget -qq https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/glove.6B.zip\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53g0zVbjRV7K"
      },
      "source": [
        "##   **Stage 1**:  Data Loading and Perform Exploratory Data Analysis (1 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq7__byEHabv",
        "outputId": "9e0c1967-7e79-40e1-9bec-292bd9773a32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train data 1044897 \n",
            "\n",
            "                    qid                                      question_text  \\\n",
            "0  2549b81c4adff1849a7f                          Is CSE at bit Meara good?   \n",
            "1  0558ed93a4630e68f7ac  Is it better to exercise before or after the b...   \n",
            "2  5d72d5233059e44f8a8e  Can character naming in writing infringe on tr...   \n",
            "3  3968636ac28841d0c901  Why does everyone making YouTube videos in Jap...   \n",
            "4  201d2b9a777bbf25443f  Is there any relation between horse power and ...   \n",
            "\n",
            "   target  \n",
            "0       0  \n",
            "1       0  \n",
            "2       0  \n",
            "3       0  \n",
            "4       0   \n",
            "\n",
            "0    980293\n",
            "1     64604\n",
            "Name: target, dtype: int64 \n",
            "\n",
            "False \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# Loading the train_dataset\n",
        "df_train = pd.read_csv('train_dataset.csv')\n",
        "print('Length of train data',len(df_train),'\\n')\n",
        "print(df_train.head(),'\\n') # looking at the data and field structure\n",
        "print(df_train['target'].value_counts(),'\\n')  #looking at the spread of target variable\n",
        "print(df_train.isnull().values.any(),'\\n') # zero null values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rxNHksrmlHT",
        "outputId": "0fdde27f-e692-4d1c-9513-037a573d7ff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     qid                                      question_text  \\\n",
            "16  8ea797496fc68c9d8d98             Why are black people always tormented?   \n",
            "28  72e1085eab12b6aa55e2                              How do you spell aye?   \n",
            "29  8137a860b078efcadd4c  Why do Conservatives want all news to be conse...   \n",
            "55  4233e8ed3bbbf5b8a242  Are we all for calling the people born in the ...   \n",
            "67  4c4e07c6a1723d0fe649  Why did the frustrated Catholics of South Indi...   \n",
            "\n",
            "    target  \n",
            "16       1  \n",
            "28       1  \n",
            "29       1  \n",
            "55       1  \n",
            "67       1  \n"
          ]
        }
      ],
      "source": [
        "print(df_train[df_train['target']==1].head()) # to see if 1 means negative or positive\n",
        "# target 1 means negative, irrelevant and inappropriate question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLeXJRVHRe_0"
      },
      "outputs": [],
      "source": [
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "# nltk.download('stopwords')\n",
        "# stopwords = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Rj4bXUftyS_x"
      },
      "outputs": [],
      "source": [
        "# removal of stop words\n",
        "def stopwordsremoval(sentence):\n",
        "  sentence = sentence.lower()\n",
        "  words = sentence.split()\n",
        "  filtered_words = [word for word in words if word.lower() not in stopwords]\n",
        "  filtered_sentence = ' '.join(filtered_words)\n",
        "  return filtered_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "XeHjy0Aomqws"
      },
      "outputs": [],
      "source": [
        "def cleaning_dataset(df):\n",
        "\n",
        "    # Pre-Processing\n",
        "    # converat all sentences to string format\n",
        "    df['question_text'] = df['question_text'].astype(str)\n",
        "\n",
        "    # convert all sentences to lower case\n",
        "    df['question_text'] = df['question_text'].apply(lambda sentence_A: sentence_A.lower())\n",
        "    # df['question_text'] = df['question_text'].apply(lambda sentence: stopwordsremoval(sentence))\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "CYufLHx3nDIe",
        "outputId": "cc94a9bc-7d44-4681-aa82-b204c5d2413f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                       question_text  target\n",
              "0                          is cse at bit meara good?       0\n",
              "1  is it better to exercise before or after the b...       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f19d40bd-2ab0-45b4-909f-78ecf511a9e0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>is cse at bit meara good?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is it better to exercise before or after the b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f19d40bd-2ab0-45b4-909f-78ecf511a9e0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f19d40bd-2ab0-45b4-909f-78ecf511a9e0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f19d40bd-2ab0-45b4-909f-78ecf511a9e0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# cleaning the questions column by lowering\n",
        "df_train_cleaned = cleaning_dataset(df_train)\n",
        "df_train_cleaned.drop(['qid'],axis=1,inplace=True)\n",
        "df_train_cleaned.head(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nlpaug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zfX38wW4mDD",
        "outputId": "917541ce-0e5e-4a27-8ad5-e77c07e05e28"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.27.1)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.65.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.11.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.4.1)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n",
            "Installing collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-2Eev5wBOn-E"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertModel, DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "import nlpaug\n",
        "import nlpaug.augmenter.word as naw\n",
        "from nlpaug.augmenter.word import SynonymAug"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GsgBf1zdIuLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RHBAyW0RIFF4"
      },
      "outputs": [],
      "source": [
        "# sentences_pos = df_train_cleaned[df_train_cleaned['target']==0]['question_text'].tolist()\n",
        "sentences_neg = df_train_cleaned[df_train_cleaned['target']==1]['question_text'].tolist()\n",
        "labels = df_train_cleaned['target'].tolist()\n",
        "# sentences_pos[0:5],sentences_neg[0:5],labels[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VjKrcTVzrL6Q"
      },
      "outputs": [],
      "source": [
        "# Define the NLP augmentation function\n",
        "def augment_sentence(sentence, num_aug=10):\n",
        "    # aug = SynonymAug(aug_max=4)\n",
        "    aug = SynonymAug(aug_max=4)\n",
        "    augmented_texts = aug.augment(sentence, n=num_aug)\n",
        "    return augmented_texts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_neg_augmented = [x for sent in sentences_neg for x in augment_sentence(sent)]\n",
        "print(sentences_neg_augmented[8:12])\n",
        "label_neg_augmented = [1 for x in sentences_neg_augmented]\n",
        "# print(label_neg_augmented[0:15],len(label_neg_augmented))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuwjjCJjIRJ2",
        "outputId": "c290363f-fcc4-4fef-a68e-60081eb3c50d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['why be black mass invariably tormented?', 'wherefore be black people incessantly tormented?', 'how do you import aye?', 'how serve you spell aye?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tot_sentences = df_train_cleaned['question_text'].tolist()\n",
        "tot_sentences.extend(sentences_neg_augmented)\n",
        "tot_labels = df_train_cleaned['target'].tolist()\n",
        "tot_labels.extend(label_neg_augmented)\n",
        "len(tot_sentences),len(tot_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjpeaVziJD4V",
        "outputId": "669c1ab4-60cc-4e10-b130-717aaea0bca8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1690937, 1690937)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict = {'question_text':tot_sentences,'target':tot_labels}\n",
        "# df_aug = pd.DataFrame(dict)\n",
        "df_train_cleaned = pd.DataFrame(dict) #temporary until we are using nlp aug instead of smote\n",
        "# len(df_aug),df_aug.tail(4)"
      ],
      "metadata": {
        "id": "Gz_ybyiLJFMM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_train_cleaned = df_aug"
      ],
      "metadata": {
        "id": "yexlXKe8JJ1D"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### finsihed nlp augmentation"
      ],
      "metadata": {
        "id": "l5vb6-GWIlOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "yzXMFeMbCtjp"
      },
      "outputs": [],
      "source": [
        "# Tokenizer class from the keras.preprocessing.text module creates a word-to-index integer dictionary\n",
        "# Vectorize the text samples\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_train_cleaned['question_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "D-KVB44PDGkA"
      },
      "outputs": [],
      "source": [
        "train_ques_seq = tokenizer.texts_to_sequences(df_train_cleaned['question_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "I9XZpEXkdXgS"
      },
      "outputs": [],
      "source": [
        "max_len = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "UymMrZNJDR2M"
      },
      "outputs": [],
      "source": [
        "# padding to 50 lengths to make uniform vectors\n",
        "max_len =80 #earlier 50 dim\n",
        "train_ques_seq = pad_sequences(train_ques_seq, maxlen=max_len, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkg7rw3VEWsJ",
        "outputId": "5178afc8-4a40-470f-8d89-f26f65258937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[    4  1247    51  1287 99143    74     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    4    17   139     2   746   181    27    83     1  4529     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]]\n"
          ]
        }
      ],
      "source": [
        "print(train_ques_seq[0:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpXRJXBRDgCW",
        "outputId": "f404f815-e731-4209-b3d5-e612d0713cbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1690937, 80)\n"
          ]
        }
      ],
      "source": [
        "print(train_ques_seq.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xMZ9Hb1l-gd"
      },
      "source": [
        "### Load the GloVe word embeddings\n",
        "Now, let us load the 50-dimensional GloVe embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNHXa8G4DMcm",
        "outputId": "83f7b798-6775-4f8f-e4c2-3c166b588a35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 64576 word vectors.\n"
          ]
        }
      ],
      "source": [
        "embeddings_index = {}\n",
        "# Loading the 300-dimensional vector of the model\n",
        "# f = open('/content/glove.6B.200d.txt') #earlier 100d\n",
        "f = open('glove.6B.200d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoOwxlsFEFdH",
        "outputId": "0ef01f9b-c820-4200-f980-6985790ed63e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "202127\n"
          ]
        }
      ],
      "source": [
        "print(len(tokenizer.word_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbUbiAQ1D5HI",
        "outputId": "1da86eb6-b12d-4ad8-d20a-4983306ac519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size 202128\n",
            "Loaded 64576 word vectors.\n",
            "(202128, 200)\n"
          ]
        }
      ],
      "source": [
        "# Adding 1 because of reversed 0 index\n",
        "words_not_found = []\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocab Size %d' % vocab_size)\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "embedding_dim = 200 #earlier 50\n",
        "\n",
        "# Create a weight matrix for words in the training data\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= vocab_size:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        words_not_found.append(word)\n",
        "\n",
        "print(embedding_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Vri_Ozy0jTO",
        "outputId": "896d62f6-67fd-4ed4-828a-e6c5bbaa581f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [-0.071549  ,  0.093459  ,  0.023738  , ...,  0.33616999,\n",
              "         0.030591  ,  0.25577   ],\n",
              "       [ 0.57345998,  0.54170001, -0.23477   , ...,  0.54417998,\n",
              "        -0.23069   ,  0.34946999],\n",
              "       ...,\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "embedding_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcJu96jyw5Wi"
      },
      "source": [
        "# **Above glove vectorization done for CNN and LSTM processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "0IZR5BtqxGw3"
      },
      "outputs": [],
      "source": [
        "X = train_ques_seq\n",
        "Y = df_train_cleaned['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUVpCwBIbvwy",
        "outputId": "9ab977e4-4e07-4e67-ea44-84dbfa504796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1521843, 80) (169094, 80) (1521843,) (169094,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=0.1, shuffle=True)\n",
        "# Check for the shape of train and test sets\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7r_OkbgLfMdO",
        "outputId": "fc97811e-98ea-4f32-9487-cfa1dbef3b6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 0,  ..., 0, 1, 0])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  45,    4,    1,  ...,    0,    0,    0],\n",
              "        [  10,   56,   75,  ...,    0,    0,    0],\n",
              "        [ 169,    3,  169,  ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [   8,    4,    3,  ...,    0,    0,    0],\n",
              "        [2114,    5, 5426,  ...,    0,    0,    0],\n",
              "        [  13,    9,    3,  ...,    0,    0,    0]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "print(torch.tensor(list(y_train)))\n",
        "torch.tensor(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pvtt1iANwjer",
        "outputId": "e771e24a-c87e-486a-e24b-994da59dc334"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1521843, 80) \n",
            " 0    882264\n",
            "1    639579\n",
            "Name: target, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "print(X_train.shape,'\\n',y_train.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5ZyjSGGw4Dq"
      },
      "outputs": [],
      "source": [
        "# Apply SMOTE to oversample the minority classes\n",
        "# smote = SMOTE(sampling_strategy={1: 700000})\n",
        "# X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "# print(len(X_train),len(X_train_resampled),y_train_resampled.shape,y_train_resampled[0:1],y_train_resampled.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_resampled, y_train_resampled = X_train, y_train #in case not running smote but just nlpaug"
      ],
      "metadata": {
        "id": "E9AncFdiObkZ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFXpF8XM7vVj"
      },
      "outputs": [],
      "source": [
        "print(X_train_resampled.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "0PMsw_Lnb39x"
      },
      "outputs": [],
      "source": [
        "# Custom dataset class\n",
        "class SentenceClassDataset(Dataset):\n",
        "    def __init__(self, questions, labels):\n",
        "        self.questions = questions\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        questions = self.questions[idx]\n",
        "        labels = self.labels[idx]\n",
        "        return questions,labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "bm1cwVkLdxmS"
      },
      "outputs": [],
      "source": [
        "# train_dataset = SentenceClassDataset(torch.tensor(X_train),torch.tensor(list(y_train)))\n",
        "train_dataset = SentenceClassDataset(torch.tensor(X_train_resampled),torch.tensor(list(y_train_resampled)))\n",
        "test_dataset = SentenceClassDataset(torch.tensor(X_test),torch.tensor(list(y_test)))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMAttnCNNSentenceClassifier_2(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_size, num_classes):\n",
        "        super(LSTMAttnCNNSentenceClassifier_2, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(embedding_matrix), freeze=True)\n",
        "\n",
        "        self.conv1 = nn.Conv1d(embedding_dim, 128, kernel_size=6)\n",
        "        self.conv2 = nn.Conv1d(128, 256, kernel_size=5)\n",
        "        self.conv3 = nn.Conv1d(256, 512, kernel_size=4)\n",
        "        self.fc1 = nn.Linear(512, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(128)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
        "        self.batch_norm3 = nn.BatchNorm1d(512)\n",
        "\n",
        "        self.lstm1 = nn.LSTM(embedding_matrix.shape[1], hidden_size,\n",
        "                             batch_first=True, bidirectional=True)\n",
        "        self.lstm2 = nn.LSTM(hidden_size*2, hidden_size,\n",
        "                             batch_first=True, bidirectional=True)\n",
        "        self.lstm3 = nn.LSTM(hidden_size*2, hidden_size,\n",
        "                             batch_first=True, bidirectional=True)\n",
        "        self.dropout_lstm = nn.Dropout(0.2)\n",
        "        self.batch_norm_lstm = nn.BatchNorm1d(hidden_size*2)\n",
        "        self.fc_lstm = nn.Linear(hidden_size*2, num_classes)\n",
        "        self.attention = nn.Linear(hidden_size*2, 1)\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, (hidden, _) = self.lstm1(embedded)\n",
        "        output = self.dropout_lstm(output)\n",
        "        output = output.permute(0, 2, 1)\n",
        "        output = self.batch_norm_lstm(output)\n",
        "        output = output.permute(0, 2, 1)\n",
        "        output, (hidden, _) = self.lstm2(output)\n",
        "        output = self.dropout_lstm(output)\n",
        "        output, (hidden, _) = self.lstm3(output)\n",
        "        output = self.dropout_lstm(output)\n",
        "        attention_weights = F.softmax(self.attention(output), dim=1)\n",
        "        context_vector = torch.sum(output * attention_weights, dim=1)\n",
        "        fc_out = self.fc_lstm(context_vector)\n",
        "        logits_lstm = self.softmax(fc_out)\n",
        "\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "        conv1_out = F.relu(self.batch_norm1(self.conv1(embedded)))\n",
        "        conv2_out = F.relu(self.batch_norm2(self.conv2(conv1_out)))\n",
        "        conv3_out = F.relu(self.batch_norm3(self.conv3(conv2_out)))\n",
        "        pooled = F.max_pool1d(conv3_out, kernel_size=conv3_out.size(2)).squeeze(2)\n",
        "        fc1_out = F.relu(self.dropout(self.fc1(pooled)))\n",
        "        fc2_out = self.fc2(fc1_out)\n",
        "        logits_cnn = self.softmax(fc2_out)\n",
        "\n",
        "        logits = (logits_lstm + logits_cnn) / 2\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "TTGIU9i08hUu"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "CZRbBjM7XJtW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the hyperparameters\n",
        "embedding_dim = 200 #earlier 50\n",
        "num_classes = 2\n",
        "vocab_size = vocab_size\n",
        "pretrained_embeddings = embedding_matrix  # Provide your pretrained GloVe embeddings\n",
        "num_epochs = 25\n",
        "batch_size = 100\n",
        "hidden_size = 64\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Create an instance of the CNN model\n",
        "# cnn_model = CNNSentenceClassifier(embedding_dim, num_classes, vocab_size, pretrained_embeddings)\n",
        "# cnn_model = CNNSentenceClassifier_v2(embedding_dim, num_classes, vocab_size, pretrained_embeddings)\n",
        "\n",
        "# Create an instance of the LSTM + Attn model\n",
        "# lstmattn_model = LSTMAttnSentenceClassifier(pretrained_embeddings,hidden_size,num_classes)\n",
        "# lstmattn_model = LSTMAttnSentenceClassifier_2(pretrained_embeddings,hidden_size,num_classes)\n",
        "\n",
        "lstmattnCNN_model = LSTMAttnCNNSentenceClassifier_2(pretrained_embeddings,hidden_size,num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "5LkDaJaq4_n4"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5I6dSwsQNWW",
        "outputId": "daaa5e06-7a98-4f79-c8bb-e51cbdceea90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMAttnCNNSentenceClassifier_2(\n",
              "  (embedding): Embedding(202128, 200)\n",
              "  (conv1): Conv1d(200, 128, kernel_size=(6,), stride=(1,))\n",
              "  (conv2): Conv1d(128, 256, kernel_size=(5,), stride=(1,))\n",
              "  (conv3): Conv1d(256, 512, kernel_size=(4,), stride=(1,))\n",
              "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (batch_norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (batch_norm3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (lstm1): LSTM(200, 64, batch_first=True, bidirectional=True)\n",
              "  (lstm2): LSTM(128, 64, batch_first=True, bidirectional=True)\n",
              "  (lstm3): LSTM(128, 64, batch_first=True, bidirectional=True)\n",
              "  (dropout_lstm): Dropout(p=0.2, inplace=False)\n",
              "  (batch_norm_lstm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc_lstm): Linear(in_features=128, out_features=2, bias=True)\n",
              "  (attention): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (softmax): Softmax(dim=None)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "# Define the loss function and optimizer\n",
        "# model = cnn_model  # if CNN model\n",
        "# model = lstmattn_model #if LSTM ATTn model\n",
        "model = lstmattnCNN_model # if LSTM Attn CNN model\n",
        "\n",
        "model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "YuiSVPFagCnk"
      },
      "outputs": [],
      "source": [
        "# Create data loader\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJh7keE707ec"
      },
      "outputs": [],
      "source": [
        "# for batch in train_data_loader:\n",
        "#     print(tuple(t.to(device) for t in batch))\n",
        "#     questions, labels = tuple(t.to(device) for t in batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj85thqsgLHX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXJmMf6PYLXB",
        "outputId": "37e2b3ed-51fe-4ef7-c65b-417271c38f1d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-52-efb4d5234af1>:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  logits_lstm = self.softmax(fc_out)\n",
            "<ipython-input-52-efb4d5234af1>:52: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  logits_cnn = self.softmax(fc2_out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "Training loss: 0.40611285707923805\n",
            "Validation loss: 0.3926773642485555\n",
            "Validation accuracy: 0.9023738275751949\n",
            "\n",
            "Epoch 2/25\n",
            "Training loss: 0.38860167005551033\n",
            "Validation loss: 0.3820148639112165\n",
            "Validation accuracy: 0.9287615172625877\n",
            "\n",
            "Epoch 3/25\n",
            "Training loss: 0.3802102619131101\n",
            "Validation loss: 0.37753739422556215\n",
            "Validation accuracy: 0.934616248950288\n",
            "\n",
            "Epoch 4/25\n",
            "Training loss: 0.37571762836953093\n",
            "Validation loss: 0.37468515737461383\n",
            "Validation accuracy: 0.9382651069819153\n",
            "\n",
            "Epoch 5/25\n",
            "Training loss: 0.3727592675379929\n",
            "Validation loss: 0.3748762077618322\n",
            "Validation accuracy: 0.9376027534980543\n",
            "\n",
            "Epoch 6/25\n",
            "Training loss: 0.3709163426162301\n",
            "Validation loss: 0.3736533962326738\n",
            "Validation accuracy: 0.9324576862573479\n",
            "\n",
            "Epoch 7/25\n",
            "Training loss: 0.36911659396323715\n",
            "Validation loss: 0.3729146044857752\n",
            "Validation accuracy: 0.9361952523448496\n",
            "\n",
            "Epoch 8/25\n",
            "Training loss: 0.3671552685562873\n",
            "Validation loss: 0.3707179412518924\n",
            "Validation accuracy: 0.9402817367854566\n",
            "\n",
            "Epoch 9/25\n",
            "Training loss: 0.3655993776754885\n",
            "Validation loss: 0.3699763676846683\n",
            "Validation accuracy: 0.9419494482358924\n",
            "\n",
            "Epoch 10/25\n",
            "Training loss: 0.36459017166899305\n",
            "Validation loss: 0.3706255667242054\n",
            "Validation accuracy: 0.9366387926242209\n",
            "\n",
            "Epoch 11/25\n",
            "Training loss: 0.363239521582843\n",
            "Validation loss: 0.3680315812722527\n",
            "Validation accuracy: 0.9426177155901452\n",
            "\n",
            "Epoch 12/25\n",
            "Training loss: 0.3618175112035602\n",
            "Validation loss: 0.3677746236042325\n",
            "Validation accuracy: 0.9435166238896708\n",
            "\n",
            "Epoch 13/25\n",
            "Training loss: 0.3610908337022121\n",
            "Validation loss: 0.3682187996553571\n",
            "Validation accuracy: 0.9418193430872769\n",
            "\n",
            "Epoch 14/25\n",
            "Training loss: 0.36064181866764244\n",
            "Validation loss: 0.3709960503207375\n",
            "Validation accuracy: 0.937129643866725\n",
            "\n",
            "Epoch 15/25\n",
            "Training loss: 0.361617248495318\n",
            "Validation loss: 0.3682889298424081\n",
            "Validation accuracy: 0.9384839201864051\n",
            "\n",
            "Epoch 16/25\n",
            "Training loss: 0.3626179843739569\n",
            "Validation loss: 0.3697799642117047\n",
            "Validation accuracy: 0.9324281169053898\n",
            "\n",
            "Epoch 17/25\n",
            "Training loss: 0.36482117503511347\n",
            "Validation loss: 0.3707244426346897\n",
            "Validation accuracy: 0.930931907696311\n",
            "\n",
            "Epoch 18/25\n",
            "Training loss: 0.36713595630477314\n",
            "Validation loss: 0.3757014752033545\n",
            "Validation accuracy: 0.9164429252368506\n",
            "\n",
            "Epoch 19/25\n",
            "Training loss: 0.3697192180394829\n",
            "Validation loss: 0.3734674987216309\n",
            "Validation accuracy: 0.9223272262765089\n",
            "\n",
            "Epoch 20/25\n",
            "Training loss: 0.37172234203680893\n",
            "Validation loss: 0.3761862781821871\n",
            "Validation accuracy: 0.9126876175381741\n",
            "\n",
            "Epoch 21/25\n",
            "Training loss: 0.3718181328948486\n",
            "Validation loss: 0.3787225366942915\n",
            "Validation accuracy: 0.9109666812542137\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example code for training the model\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "\n",
        "  # Training loop\n",
        "  for batch in train_data_loader:\n",
        "    # print(tuple(t.to(device) for t in batch))\n",
        "    inputs, labels = tuple(t.to(device) for t in batch)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)  # inputs is your input sentence tensor\n",
        "    loss = criterion(outputs, labels)  # labels is your target class tensor\n",
        "    total_loss += loss.item()\n",
        "    # print('batch loss',loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # Perform evaluation, validation, or other tasks as needed\n",
        "\n",
        "  # Calculate average training loss\n",
        "  avg_train_loss = total_loss / len(train_data_loader)\n",
        "\n",
        "  # Validation loop\n",
        "  model.eval()\n",
        "  val_loss = 0\n",
        "  val_accuracy = 0\n",
        "  val_steps = 0\n",
        "  total_f1_score = 0\n",
        "# Example code for inference/prediction\n",
        "  for batch in test_data_loader:\n",
        "    inputs, labels = tuple(t.to(device) for t in batch)\n",
        "    with torch.no_grad():\n",
        "      outputs = model(inputs)  # inputs is your input sentence tensor\n",
        "      loss = criterion(outputs, labels)\n",
        "      val_loss += loss.item()\n",
        "      # print('Val Loss',loss.item())\n",
        "      logits = outputs\n",
        "      _, predictions = torch.max(logits, dim=1)\n",
        "      val_accuracy += torch.sum(predictions == labels).item()\n",
        "      val_steps += labels.size(0)\n",
        "      # f1 = f1_score(labels, predictions) # while gpu commenting out\n",
        "      # print(\"F1 score:\", f1) #while gpu commenting out\n",
        "      # total_f1_score += f1 #while gpu commenting out\n",
        "\n",
        "  # Calculate average validation loss and accuracy\n",
        "  avg_val_loss = val_loss / len(test_data_loader)\n",
        "  avg_val_accuracy = val_accuracy / val_steps\n",
        "  # avg_total_f1_score = total_f1_score/val_steps #while gpu commenting out\n",
        "\n",
        "  print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "  print(f'Training loss: {avg_train_loss}')\n",
        "  print(f'Validation loss: {avg_val_loss}')\n",
        "  print(f'Validation accuracy: {avg_val_accuracy}')\n",
        "  # print(f'Validation F1 Score: {avg_total_f1_score}') # while gpu commenting out\n",
        "  print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_61JDIcHSgl"
      },
      "outputs": [],
      "source": [
        "print(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x5FEcICIoH4"
      },
      "outputs": [],
      "source": [
        "len([x for x in predictions if x==1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5wJrWRL8v7Q"
      },
      "outputs": [],
      "source": [
        "# saving the model, hyperparameters and embedding matrix as pretrained embeddings\n",
        "torch.save(model, 'cnnlstm_model_v61.pth')  # saving the model parameters\n",
        "# torch.save(model.state_dict(), 'cnn_model.pth')  # saving the model parameters\n",
        "\n",
        "hyperparameters = {\n",
        "    'embedding_dim': embedding_dim,\n",
        "    'hidden_dim': hidden_size,\n",
        "    'num_classes': num_classes,\n",
        "    # 'dropout': dropout,\n",
        "    'learning_rate': learning_rate,\n",
        "    'vocab_size': vocab_size,\n",
        "    'pretrained_embeddings':pretrained_embeddings,\n",
        "    'num_epochs':num_epochs,\n",
        "    'batch_size': batch_size,\n",
        "    'max_token_len': max_len\n",
        "}\n",
        "torch.save(hyperparameters, 'lstmcnn_hyperparameters_v61.pth')  #saving the model hyperparameters\n",
        "\n",
        "np.save('lstmcnn_pretrained_embeddings_v61.npy', pretrained_embeddings) #saving the pretrained glove embeddings as embedding matrix\n",
        "# saving fitted tokenizer\n",
        "with open('lstmcnn_tokenizer_v61.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x7w9qEQOgar"
      },
      "source": [
        "***For Prediction***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk-DkQFmsgTn"
      },
      "outputs": [],
      "source": [
        "# # Function for inference/prediction on a new dataset\n",
        "# def preprocess_input(input_sentences):\n",
        "#     input_tensor = []\n",
        "#     for sentence in input_sentences:\n",
        "#         word_indices = []\n",
        "#         for word in sentence.split():\n",
        "#             if word in word_to_index:\n",
        "#                 word_indices.append(word_to_index[word])\n",
        "#             else:\n",
        "#                 word_indices.append(0)  # Assign index 0 for unknown words\n",
        "#         input_tensor.append(word_indices)\n",
        "#     input_tensor = torch.LongTensor(input_tensor)\n",
        "#     return input_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_5FaYpbcriA"
      },
      "outputs": [],
      "source": [
        "## Loading all the model params\n",
        "def load_model():\n",
        "  model = torch.load('cnnlstm_model_v4.pth') # keep the model class defined as global\n",
        "  hyperparameters = torch.load('lstmcnn_hyperparameters_v4.pth')\n",
        "  pretrained_embeddings = np.load('lstmcnn_pretrained_embeddings_v4.npy')\n",
        "  with open('lstmcnn_tokenizer_v4.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "  return(model,hyperparameters,pretrained_embeddings,tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfpfP09KdNVj"
      },
      "outputs": [],
      "source": [
        "## Run class CNNSentenceClassifier(nn.Module) block above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0bHXZhQczon"
      },
      "outputs": [],
      "source": [
        "model,hyperparameters,pretrained_embeddings,tokenizer = load_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1LHv2zvdwJX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01e03945-96f6-4480-b029-24e7e198dfa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'embedding_dim': 200, 'hidden_dim': 64, 'num_classes': 2, 'learning_rate': 0.001, 'vocab_size': 202061, 'pretrained_embeddings': array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
            "         0.        ,  0.        ],\n",
            "       [-0.071549  ,  0.093459  ,  0.023738  , ...,  0.33616999,\n",
            "         0.030591  ,  0.25577   ],\n",
            "       [ 0.57345998,  0.54170001, -0.23477   , ...,  0.54417998,\n",
            "        -0.23069   ,  0.34946999],\n",
            "       ...,\n",
            "       [ 0.17603   ,  0.061422  ,  0.080036  , ...,  0.041023  ,\n",
            "         0.33221999, -0.1688    ],\n",
            "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
            "         0.        ,  0.        ],\n",
            "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
            "         0.        ,  0.        ]]), 'num_epochs': 5, 'batch_size': 100, 'max_token_len': 80}\n"
          ]
        }
      ],
      "source": [
        "print(hyperparameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdHA4CkIOe4G"
      },
      "outputs": [],
      "source": [
        "df_pred = pd.read_csv('test_dataset.csv')\n",
        "df_pred.head(3)\n",
        "print(df_pred.head(3),'\\n',df_pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbPL32TYdbF7"
      },
      "outputs": [],
      "source": [
        "## Run cleaning_dataset function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPh9E9nuUGVH"
      },
      "outputs": [],
      "source": [
        "# cleaning the questions column by lowering\n",
        "df_pred_cleaned = cleaning_dataset(df_pred)\n",
        "# df_pred_cleaned.drop(['qid'],axis=1,inplace=True)\n",
        "df_pred_cleaned.head(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIcMPwgsUGVJ"
      },
      "outputs": [],
      "source": [
        "pred_ques_seq = tokenizer.texts_to_sequences(df_pred_cleaned['question_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFKqny10UGVJ"
      },
      "outputs": [],
      "source": [
        "# padding to 50 lengths to make uniform vectors\n",
        "max_token_len = hyperparameters['max_token_len']\n",
        "max_token_len = 80\n",
        "pred_ques_seq = pad_sequences(pred_ques_seq, maxlen=max_token_len, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjaeeMilUf1d"
      },
      "outputs": [],
      "source": [
        "print(pred_ques_seq.shape,pred_ques_seq[0:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tukf50YYPrS"
      },
      "outputs": [],
      "source": [
        "pred_ques_seq = torch.tensor(pred_ques_seq)\n",
        "print(pred_ques_seq.shape,pred_ques_seq[0:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZ7w6nDH2qr3"
      },
      "outputs": [],
      "source": [
        "pred_batch_size = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vG5YDSTpyIVh"
      },
      "outputs": [],
      "source": [
        "# Custom dataset class\n",
        "class PredSentenceClassDataset(Dataset):\n",
        "    def __init__(self, questions):\n",
        "        self.questions = questions\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        questions = self.questions[idx]\n",
        "        return questions\n",
        "pred_dataset = PredSentenceClassDataset(pred_ques_seq)\n",
        "pred_data_loader = torch.utils.data.DataLoader(pred_dataset, batch_size=pred_batch_size, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiZZURZL2tgh"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "final_pred_class_value = []\n",
        "for batch_questions in pred_data_loader:\n",
        "  print(batch_questions.shape)\n",
        "  batch_questions = batch_questions.to(device)\n",
        "  outputs = model(batch_questions)\n",
        "  _, pred_class_value = torch.max(outputs, dim=1)\n",
        "  final_pred_class_value.extend(pred_class_value)\n",
        "  print(len(final_pred_class_value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxAFhl-dYqJt"
      },
      "outputs": [],
      "source": [
        "# pred_batch_size = 5000\n",
        "# final_pred_class_value = []\n",
        "# total_pred_run = int(len(pred_ques_seq)/pred_batch_size)\n",
        "# print(total_pred_run)\n",
        "# for i in range(total_pred_run):\n",
        "#   # print(i)\n",
        "#   if(i<total_pred_run-1):\n",
        "#     outputs = model(pred_ques_seq[i*pred_batch_size:(i+1)*pred_batch_size-1,:])\n",
        "#   else:\n",
        "#     outputs = model(pred_ques_seq[i*pred_batch_size:])\n",
        "\n",
        "#   print(i*pred_batch_size,(i+1)*pred_batch_size-1)\n",
        "\n",
        "#   _, pred_class_value = torch.max(outputs, dim=1)\n",
        "#   final_pred_class_value.extend(pred_class_value)\n",
        "#   print(len(final_pred_class_value))\n",
        "\n",
        "# # print(len(final_pred_class_value),final_pred_class_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ek-NsCN0Z6bR"
      },
      "outputs": [],
      "source": [
        "# print(final_pred_class_value)\n",
        "print([x for x in final_pred_class_value if x == 1])\n",
        "final_pred_class_value = [int(x) for x in final_pred_class_value]\n",
        "final_pred_class_value[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pG_jRpemai6o"
      },
      "outputs": [],
      "source": [
        "# print(df_pred.head(2))\n",
        "df_pred['target'] = pd.Series(final_pred_class_value)\n",
        "df_pred.tail(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MS2cRJuWpag"
      },
      "outputs": [],
      "source": [
        "df_pred[df_pred['target']==1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3r8lXImkGjf"
      },
      "outputs": [],
      "source": [
        "df_pred[['qid','target']].to_csv('Group4_Pred_Submission_v51_1706_lC.csv')\n",
        "# df_pred.to_csv('Group4_Pred_Submission_v75_lC_withques.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuVVF2-cFtR1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# submit the file to kaggle\n",
        "!kaggle competitions submit toxic-questions-classification -f Group4_Pred_Submission_v51_1706_lC.csv -m \"Model dr+bn+cn+mk ep=8, bt_sz=100, cn_lay=3,4,5 dropout=0.3, logit max\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "CAHX2GaOEf-v",
        "outputId": "ff3dedf2-1565-4ed4-d31a-00ffd3659192"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b138b520-9978-4c05-bb0b-70e19a590b74\", \"Group4_Pred_Submission_v51_1406_lC.csv\", 7725276)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "# files.download('Group4_Pred_Submission_v75_lC_withques.csv')\n",
        "files.download('Group4_Pred_Submission_v51_1406_lC.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBlwD32hwutx"
      },
      "source": [
        "# **FOR BERT PROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLyVJry2PjJH"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertModel, DistilBertTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBHbF2zWCyG6"
      },
      "outputs": [],
      "source": [
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Assuming binary classification (appropriate vs. inappropriate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huj8AWOBPnGw"
      },
      "outputs": [],
      "source": [
        "# Load the DistilBERT pre-trained model and tokenizer\n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "model = DistilBertModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prYdffVN-_vs"
      },
      "outputs": [],
      "source": [
        "# df_train_cleaned\n",
        "\n",
        "category_A_limit = 100000  # Specify the desired limit for category A\n",
        "category_A = 0  # Specify the target category appropriate = 0\n",
        "\n",
        "# Filter the DataFrame to keep only rows with target category A\n",
        "df_category_A = df_train_cleaned[df_train_cleaned['target'] == category_A]\n",
        "\n",
        "# Shuffle the filtered DataFrame\n",
        "df_category_A = df_category_A.sample(frac=1, random_state=42)\n",
        "\n",
        "# Keep only the limited number of rows for category A\n",
        "df_category_A = df_category_A[:category_A_limit]\n",
        "\n",
        "# Filter out the rows with target category A from the original DataFrame\n",
        "df_other_category = df_train_cleaned[df_train_cleaned['target'] != category_A]\n",
        "\n",
        "# Concatenate the limited category A DataFrame with the other category DataFrame\n",
        "df_final = pd.concat([df_category_A, df_other_category], ignore_index=True)\n",
        "\n",
        "# Shuffle the final DataFrame\n",
        "df_final = df_final.sample(frac=1, random_state=42)\n",
        "print(df_final.shape,df_final.head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTk6N124DauT"
      },
      "outputs": [],
      "source": [
        "df_train_cleaned=df_final\n",
        "print(df_train_cleaned.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRXGKyoAxI6h"
      },
      "outputs": [],
      "source": [
        "print(list(df_train_cleaned['question_text'])[0:2])\n",
        "ques_sentence_data = list(df_train_cleaned['question_text'])\n",
        "labels = list(df_train_cleaned['target'])\n",
        "print(labels[0:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF59X-qeDBZR"
      },
      "outputs": [],
      "source": [
        "# Tokenize the sentence pairs and encode labels\n",
        "def bert_tokenize(ques_sentence_data,labels):\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  for ques in ques_sentence_data:\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "          ques,\n",
        "          add_special_tokens=True,\n",
        "          max_length=60, # default 128\n",
        "          padding='max_length',\n",
        "          truncation=True,\n",
        "          return_attention_mask=True,\n",
        "          return_tensors='pt'\n",
        "      )\n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "  return(input_ids, attention_masks,labels)\n",
        "\n",
        "# # Create a DataLoader for the dataset\n",
        "# dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "# batch_size = 40\n",
        "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AJoqDMg2vQs"
      },
      "outputs": [],
      "source": [
        "# PREPROCESS your dataset by encoding BERT wise, do this for prediction data as well\n",
        "input_ids, attention_masks,labels = bert_tokenize(ques_sentence_data,labels)\n",
        "print(input_ids[0:2],labels[0:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sksloAArDCUg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, test_size=0.2, random_state=42)\n",
        "train_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPI4A6BWdMSv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae3jDzrd4Vkh"
      },
      "outputs": [],
      "source": [
        "# Define batch size and create DataLoader\n",
        "batch_size = 4000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBcumQw7DFZv"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Afne2UScDITU"
      },
      "outputs": [],
      "source": [
        "# Set up optimizer and training parameters\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "num_epochs = 2\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AGvw9vbQGOH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Before training loop\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiAI2aqXDL92"
      },
      "outputs": [],
      "source": [
        "# Fine-tuning loop\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    # Training loop\n",
        "    for batch in train_dataloader:\n",
        "        batch_inputs, batch_masks, batch_labels = tuple(t.to(device) for t in batch)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=batch_inputs, attention_mask=batch_masks)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # outputs = model(input_ids=batch_inputs, attention_mask=batch_masks, labels=batch_labels)\n",
        "        # loss = outputs.loss\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate average training loss\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_accuracy = 0\n",
        "    val_steps = 0\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "        batch_inputs, batch_masks, batch_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          outputs = model(input_ids=batch_inputs, attention_mask=batch_masks)\n",
        "\n",
        "            # outputs = model(input_ids=batch_inputs, attention_mask=batch_masks, labels=batch_labels)\n",
        "\n",
        "        val_loss += outputs.loss.item()\n",
        "\n",
        "        logits = outputs.logits\n",
        "        _, predictions = torch.max(logits, dim=1)\n",
        "        val_accuracy += torch.sum(predictions == batch_labels).item()\n",
        "        val_steps += batch_labels.size(0)\n",
        "\n",
        "    # Calculate average validation loss and accuracy\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    avg_val_accuracy = val_accuracy / val_steps\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "    print(f'Training loss: {avg_train_loss}')\n",
        "    print(f'Validation loss: {avg_val_loss}')\n",
        "    print(f'Validation accuracy: {avg_val_accuracy}')\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oLyIb19KcdL"
      },
      "source": [
        "##   **Stage 2**: Data Pre-Processing  (1 Points)\n",
        "\n",
        "####  Clean and Transform the data into a specified format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6ZCiIxxKiq7"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jZg7yL2TtTM"
      },
      "source": [
        "##   **Stage 3**: Build the Word Embeddings using pretrained Word2vec/Glove (Text Representation) (1 Point)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9rTpuiwSy0S"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6jfm3YFUL7i"
      },
      "source": [
        "##   **Stage 4**: Build and Train the Deep networks model using Pytorch/Keras (5 Points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz1-Bs4pUdsf"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-O0Jx99UhmI"
      },
      "source": [
        "##   **Stage 5**: Evaluate the Model and get model predictions on the test dataset (2 Points)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSaAlhGGUitF"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}